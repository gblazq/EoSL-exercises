{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "**Suppose each of $K$-classes has an associated target $t_k$ , which is a vector of all zeros, except a one in the $k$th position. Show that classifying to the largest element of $\\hat y$ amounts to choosing the closest target,\n",
    "$$\\min_{k}\\vert\\vert t_k âˆ’ \\hat y\\vert\\vert$$\n",
    "if the elements of $\\hat y$ sum to one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between $t_k$ and $\\hat y$ is \n",
    "$$\\vert\\vert t_k - \\hat y \\vert\\vert = \\sqrt{\\sum_i\\left(t_{k_i} - \\hat y_i\\right)^2} = \\sqrt{\\sum_{i\\neq k}\\hat y_i^2 + \\left(1 - \\hat y_k\\right)^2} = \\sqrt{1+ \\sum_i \\hat y_i^2 - 2\\hat y_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $1 + \\sum_i \\hat y_i^2$ is the same for all $k$, $\\hat y$ is closest to the $t_k$ target for which $\\hat y_k$ is greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful in categorical predictions. For example, in letter recognition, we can generate a vector output of dimensionality equal to the number of letters in the alphabet and assign the input to the letter corresponding to the largest output value. It is not necessary that the elements of $\\hat y$ sum to one, but if they do we can interpret the $y_k$ value as the probability of the input to belong to class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "**Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92b13882b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed, randint, rand\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed(2666)\n",
    "\n",
    "blue_m = [1,0]\n",
    "orange_m = [0,1]\n",
    "cov = np.eye(2)\n",
    "cov_points = cov/5\n",
    "n_means = 10\n",
    "\n",
    "# Generate the 10 means for each class\n",
    "blue_means = multivariate_normal(blue_m, cov).rvs(n_means)\n",
    "orange_means = multivariate_normal(orange_m, cov).rvs(n_means)\n",
    "\n",
    "\n",
    "# Generate the 100 points for each class:\n",
    "# - Draw a mean from the 10 possible means with P=1/10\n",
    "# - Draw a point from a multivariate normal with that mean and cov = eye()/5\n",
    "blue_points, orange_points = np.empty([100, 2]), np.empty([100, 2])\n",
    "\n",
    "for i in range(100):\n",
    "    k_blue, k_orange = randint(n_means), randint(n_means)\n",
    "    blue_points[i, :] = multivariate_normal(blue_means[k_blue, :], cov_points).rvs()\n",
    "    orange_points[i, :] = multivariate_normal(orange_means[k_orange, :], cov_points).rvs()\n",
    "\n",
    "# Define the function of the Bayes optimal decision boundary\n",
    "def blue_pdf(x):\n",
    "    pdf = 0\n",
    "    for i in range(10):\n",
    "        pdf += multivariate_normal(blue_means[i,:], cov_points).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "def orange_pdf(x):\n",
    "    pdf = 0\n",
    "    for i in range(10):\n",
    "        pdf += multivariate_normal(orange_means[i,:], cov_points).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "def pdf_subtract(x):\n",
    "    p = blue_pdf(x) - orange_pdf(x)\n",
    "    return p\n",
    "\n",
    "# Compute the irreducible error\n",
    "correct = 0\n",
    "total = 3000\n",
    "for i in range(total):\n",
    "    # Select the mean\n",
    "    mean = randint(n_means)\n",
    "    if rand() > 0.5:\n",
    "        mean = blue_means[mean]\n",
    "        color = 0\n",
    "    else:\n",
    "        mean = orange_means[mean]\n",
    "        color = 1\n",
    "\n",
    "    # Draw a point\n",
    "    point = multivariate_normal(mean, cov_points).rvs()\n",
    "\n",
    "    # Predict the color\n",
    "    if pdf_subtract(point) > 0:\n",
    "        predicted_color = 0\n",
    "    else:\n",
    "        predicted_color = 1\n",
    "\n",
    "    if predicted_color == color:\n",
    "        correct += 1\n",
    "\n",
    "irr_error_rate = 1 - correct/total\n",
    "\n",
    "# Plot the means, points and Bayes boundary\n",
    "x = np.arange(-4,4,0.025)\n",
    "y = np.arange(-4,4,0.025)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "plt.plot(blue_means[:,0], blue_means[:,1], 'bx')\n",
    "plt.plot(orange_means[:,0], orange_means[:,1], 'rx')\n",
    "plt.plot(blue_points[:, 0], blue_points[:, 1], 'bo')\n",
    "plt.plot(orange_points[:, 0], orange_points[:, 1], 'ro')\n",
    "plt.contourf(X, Y, pdf_subtract(np.dstack((X,Y))), levels=[-np.Inf, 0, np.Inf],\n",
    "colors=('red', 'blue'), alpha=0.25)\n",
    "plt.xlim([-4, 3.975])\n",
    "plt.ylim([-4, 3.975])\n",
    "plt.xlabel('Irreducible error rate: ' + str(irr_error_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "**Derive equation (2.24) for the median distance from the origin to the closest data point with $N$ data points uniformly distributed in a $p$-dimensional unit ball centered at the origin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $V_p(1)$ be the volume of a $p$-dimensional unit ball. Then, the volume of a ball of radius $R$ is $V_p(R) = V_p(1) R^p$.\n",
    "\n",
    "The probability that a uniformly drawn random point will have a distance $D > d$ is proportional to the volume difference $V_p(1)(1-d^p)$. Normalizing it, we find $P(D > d) = 1-d^p$.\n",
    "\n",
    "Since each random point is drawn independently from the others,\n",
    "\n",
    "$$P(\\text{all }N\\text{ distances} > d) = P(\\text{closest distance} > d) = (1-d^p)^N.$$\n",
    "\n",
    "Then, the CDF is \n",
    "\n",
    "$$P(\\text{closest distance} \\leq d) = 1 - (1-d^p)^N,$$\n",
    "\n",
    "and the median is the value $d$ such that the CDF is $1/2$, which is \n",
    "\n",
    "$$d = \\left(1-\\frac{1}{2}^{1/N}\\right)^{1/p}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we need to use $P(D > d) = 1-d^p$ because $P(\\text{all }N\\text{ distances} > d) = P(\\text{closest distance} > d)$. With  $P(D < d) = d^p$ we could compute $P(\\text{all }N\\text{ distances} < d) = P(\\text{furthest distance} < d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the CDF, we can also compute the mean. The PDF is (let now $x$ be the distance)\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{d}{dx} F(x) = pN\\left(1-x^p\\right)^{N-1}x^{p-1}\n",
    "\\end{equation}\n",
    "\n",
    "and the mean is\n",
    "\n",
    "\\begin{align}\n",
    "E[\\text{closest distance}] &= \\int_0^1{x f(x) dx} = \\int_0^1{pN\\left(1-x^p\\right)^{N-1}x^p dx} \\\\\n",
    "&= \\frac{N \\Gamma (N) \\Gamma \\left(\\frac{1}{p}\\right)}{p \\Gamma \\left(N+\\frac{1}{p}+1\\right)}\n",
    "\\end{align}\n",
    "\n",
    "For example, for $N = 500$ and $p = 10$, the median is 0.5178 and the mean 0.5110."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.4\n",
    "**The edge effect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution $X \\sim N(0, \\mathbb{I}_p)$. The squared distance from any sample point to the origin has a $\\chi^2_p$ distribution with mean $p$.\n",
    "Consider a prediction point $x_0$ drawn from this distribution, and let $a = x_0 / \\vert\\vert x_0 \\vert \\vert$ be an associated unit vector.**\n",
    "\n",
    "**Let $z_i = a^T x_i$ be the projection of each of the training points on this direction. Show that the $z_i$ are distributed $N(0, 1)$ with expected squared distance from the origin 1, while the target point has expected squared distance $p$ from the origin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the points are drawn from a spherical multinormal distribution, each of the components of a random vector $x_i$ are normally distributed\n",
    "\\begin{equation}\n",
    "x_i^j \\sim N(0, 1).\n",
    "\\end{equation}\n",
    "But due to the spherical symmetry of the distribution, we can rotate the axes, so that any of the basis unit vectors coincides with $a$. For example, we can make $e_1 = a$. Then, \n",
    "\\begin{equation}\n",
    "z_i = a^T x_i = x_i^1 \\sim N(0, 1).\n",
    "\\end{equation}\n",
    "Since the squared norm of $k$ standard normally distributed variables is a chi-squared distribution with k degrees of freedom, with mean $k$, the expected squared distance from the origin is $1$, while the target point has expected squared distance $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hence for $p = 10$, a randomly drawn test point is about $3.1$ standard\n",
    "deviations from the origin, while all the training points are on average\n",
    "one standard deviation along direction $a$. So most prediction points see\n",
    "themselves as lying on the edge of the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.5\n",
    "**(a) Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation we want to derive is the EPE of a regression line fitted by least squares when the actual outputs are given by\n",
    "$y_i = x_i^T\\beta + \\varepsilon_i$, with $\\varepsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "Then, the training set $\\mathcal{T}$ is given by the input matrix $X$ and the outputs $Y = X\\beta + \\varepsilon$ or, equivalently, by $X$ and $\\varepsilon$. $X$ is a $N\\times p$ matrix, $\\beta$ is a $p\\times 1$ vector, and $Y$ and $\\varepsilon$ are $N\\times 1$ vectors. $N$ is the number of observations and $p$ the total number of parameters, including the constant.\n",
    "\n",
    "Since $\\hat \\beta$ is found by least squares, it is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\beta = (X^T X)^{-1}X^TY = \\beta + (X^T X)^{-1}X^T\\varepsilon\n",
    "\\end{equation}\n",
    "\n",
    "so we can write\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat y_0 = x_0^T\\hat \\beta = x_0^T\\beta + \\sum_{i=1}^N l_i(x_0)\\varepsilon_i\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "l_i(x_0) = \\left[X(X^TX)^{-1}x_0\\right]_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to know the EPE as a function of $x_0$. The random variables are the training set $\\mathcal{T}$ and the output $y_0 = x_0^T\\beta + \\varepsilon_0$. We first write the two expected values explicitly, conditioning $y_0$ on $x_0$ and use the usual trick of adding and subtracting the expected value of a random variable to obtain the variance and the bias. Following the same notation of the book, we will write the conditional expected value of a general random value as $E_{y_0 \\vert x_0}\\left[f(y_0)\\right]$ and as $E\\left[y_0 \\vert x_0 \\right]$ when $f(y_0) = y_0$.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}(x_0) &= E(y_0 - \\hat y_0)^2 = E_{y_0 \\vert x_0} E_\\mathcal{T} (y_0 - \\hat y_0)^2 \\\\\n",
    "&= E_{y_0 \\vert x_0} E_\\mathcal{T} \\left(y_0 - E(y_0 \\vert x_0) + E(y_0 \\vert x_0) - \\hat y_0\\right)^2 \\\\\n",
    "&= E_{y_0 \\vert x_0} E_\\mathcal{T} \\left[\\left( y_0 - E(y_0 \\vert x_0)\\right)^2 + \\left( E(y_0 \\vert x_0) - \\hat y_0\\right)^2  + 2\\left( y_0 - E(y_0 \\vert x_0)\\right)\\left( E(y_0 \\vert x_0) - \\hat y_0\\right) \\right]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last term is $2E_{y_0 \\vert x_0}\\left(y_0 - E(y_0 \\vert x_0)\\right)E_\\mathcal{T}\\left(E(y_0 \\vert x_0) - \\hat y_0\\right)$ which is 0 due to the first expected value. Moreover, the first term does not depend on the training set $\\mathcal{T}$, so it becomes a variance. In the second term we can use the same trick,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}(x_0) &= E_{y_0 \\vert x_0}\\left[ y_0 - E(y_0 \\vert x_0)\\right]^2 + E_\\mathcal{T}\\left[ E(y_0 \\vert x_0) - \\hat y_0\\right]^2 \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + E_\\mathcal{T}\\left[ \\hat y_0 - E_\\mathcal{T}(\\hat y_0) + E_\\mathcal{T}(\\hat y_0) - E(y_0 \\vert x_0)\\right]^2 \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + E_\\mathcal{T}\\left[\\hat y_0 - E_\\mathcal{T}\\left(\\hat y_0\\right)\\right]^2 + E_\\mathcal{T}\\left[E_\\mathcal{T}\\left(\\hat y_0\\right) - E(y_0 \\vert x_0) \\right]^2 \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + E_\\mathcal{T}\\left[\\hat y_0 - E_\\mathcal{T}\\left(\\hat y_0\\right)\\right]^2 + E_\\mathcal{T}^2\\left[\\hat y_0 - x_0^T\\beta \\right] \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + \\text{Var}(\\hat y_0) + \\text{Bias}^2(\\hat y_0)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last equality we have used the definition of the bias of an estimator, $\\text{Bias}(\\hat \\theta) = E\\left[\\hat \\theta - \\theta\\right]$. It is sometimes written as $\\text{Bias}(\\hat \\theta) = E\\left[\\hat \\theta\\right] - \\theta$ but *I think* the former equality is more general, and the latter is only true if $\\theta$ is not a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is\n",
    "$$\n",
    "\\text{Var}(y_0 \\vert x_0) = \\text{Var}(x_0^T\\beta + \\varepsilon \\,\\vert\\, x_0) = \\text{Var}(\\varepsilon) = \\sigma^2\n",
    "$$\n",
    "The second one is, using the law of total variance,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}_\\mathcal{T}\\left(\\hat y_0\\right) &= \\text{Var}_\\mathcal{T}\\left(x_0^T \\hat \\beta \\right) \\\\\n",
    "&= E_X\\left[\\text{Var}_\\mathcal{\\varepsilon}\\left[x_0^T\\hat\\beta \\,\\vert\\, X\\right]\\right] + \\text{Var}_X\\left[E_\\mathcal{\\varepsilon}\\left[x_0^T\\hat\\beta \\,\\vert\\, X\\right]\\right] \\\\\n",
    "&= E_X\\left[x_0^T \\left(X^T X\\right)^{-1} x_0 \\sigma^2\\right]\n",
    "\\end{align}\n",
    "\n",
    "In the last equality we have used the expression of $\\hat \\beta$ in terms of $X$ and $\\varepsilon$ and Eq. (3.8) from the book. The last term vanishes because $E(\\varepsilon) = 0$.\n",
    "Finally, the last term in the expression for the EPE is\n",
    "\n",
    "$$\n",
    "\\text{Bias}^2\\left(\\hat y_0\\right) = \\left(E\\left[\\hat y_0\\right] - x_0^T \\beta\\right)^2 = \\left(x_0^T E\\left[\\hat \\beta\\right] - x_0^T \\beta\\right)^2\n",
    "$$\n",
    "\n",
    "Using again the expression for $\\hat \\beta$ in terms of $X$ and $\\varepsilon$ and conditioning on $X$,\n",
    "\n",
    "$$\n",
    "E\\left[\\hat \\beta\\right] = E\\left[\\beta + \\left(X^T X\\right)^{-1}\\varepsilon\\right] = \\beta + E_X\\left[E_\\varepsilon\\left[\\left(X^TX\\right)^{-1}X^T\\varepsilon \\,\\vert\\, X\\right]\\right] = \\beta\n",
    "$$\n",
    "\n",
    "Then, the bias is 0 and the EPE is\n",
    "\n",
    "\\begin{align}\n",
    "EPE(x_0) &= \\text{Var}(y_0 \\vert x_0) + \\text{Var}(\\hat y_0) + \\text{Bias}^2(\\hat y_0) \\\\\n",
    "&= \\sigma^2 + E_X\\left[x_0^T \\left(X^T X\\right)^{-1} x_0 \\sigma^2\\right]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(b) Derive equation (2.28), making use of the cyclic property of the trace operator [trace(AB) = trace(BA)], and its linearity (which allows us to interchange the order of trace and expectation).**\n",
    "\n",
    "Remember that $X$ is an $N\\times p$ matrix,\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "x^T_1 \\\\ x^T_2 \\\\ \\vdots \\\\ x^T_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "X^TX = \\begin{pmatrix}\n",
    "x_1 & x_2 & \\ldots & x_N\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x^T_1 \\\\ x^T_2 \\\\ \\vdots \\\\ x^T_N\n",
    "\\end{pmatrix} = \\sum_{i=1}^N{x_i x^T_i}\n",
    "$$\n",
    "\n",
    "The covariance matrix of a random variable $Z$ is defined as $\\text{Cov}(Z) = E(ZZ^T) - E(Z)E(Z^T)$. Since we are assuming $E(x) = 0$, and all the $x$ vectors are drawn from the same distribution,\n",
    "\n",
    "$$\n",
    "E(X^TX) = NE(x x^T) = N\\text{Cov}(x)\n",
    "$$\n",
    "\n",
    "**Warning: in general, $E(f(X)) \\neq f(E(X))$, so in principle $E\\left[(X^TX)^{-1}\\right] \\neq \\left[E(X^TX)\\right]^{-1}$, but it seems that's what they are doing in the book. It could be related to the fact that we are taking the $N\\rightarrow \\infty$ limit, but I'm not sure.\n",
    "The solution may go as follows:\n",
    "**\n",
    "\n",
    "$$\n",
    "E\\left[(X^TX)^{-1}\\right] = E\\left[\\left(N xx^T\\right)^{-1}\\right]\n",
    "$$\n",
    "\n",
    "$xx^T$ is positive definite, which *probably* means that we can use some sort of Taylor expansion with leading term $1/E(Nxx^T)$ and higher order terms suppresed by powers of 1/N. This is suggested [here](https://stats.stackexchange.com/a/80884) but there are also commments arguing against it.\n",
    "\n",
    "In any case, assuming the approximation is valid,\n",
    "\n",
    "\\begin{align}\n",
    "E_{x_0} EPE(x_0) &= E_{x_0}\\left[\\sigma^2 + E\\left[x_0^T\\left(X^TX\\right)^{-1} x_0 \\sigma^2\\right]\\right] \\\\\n",
    "&\\simeq \\sigma^2 + \\frac{\\sigma^2}{N}E_{x_0}\\left[x_0^T\\text{Cov}(x)^{-1}x_0\\right]\n",
    "\\end{align}\n",
    "\n",
    "Since $x_0^T\\text{Cov}(x)^{-1}x_0$ is a scalar, it is its own trace. Using the cyclic property of the trace and its linearity\n",
    "\n",
    "$$\n",
    "E_{x_0}\\left[x_0^T\\text{Cov}(x)^{-1}x_0\\right] = E_{x_0}\\left[\\text{Tr}\\left(\\text{Cov}(x)^{-1}x_0x_0^T\\right)\\right] = \\text{Tr}\\left[\\text{Cov}(x)^{-1} E_{x_0}\\left(x_0 x_0^T\\right)\\right] = \\text{Tr}\\left[\\text{Cov}(x)^{-1}\\text{Cov}(x_0)\\right]\n",
    "$$\n",
    "\n",
    "Since the training points $x$ and the test point $x_0$ are drawn from the same distribution, we are left with the trace over the $p\\times p$ identity matrix, which is $p$, so\n",
    "\n",
    "$$\n",
    "E_{x_0} EPE(x_0) \\simeq \\sigma^2 + \\frac{\\sigma^2}{N} p\n",
    "$$\n",
    "\n",
    "for large $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "**Consider a regression problem with inputs $x_i$ and outputs $y_i$, and a parameterized model $f_\\theta(x)$ to be fit by least squares. Show that if there are observations with *tied* or *identical* values of $x$, then the fit can be obtained from a reduced weighted least squares problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is independent of the model $f_\\theta$, as long as we fit $\\hat \\theta$ by least squares:\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{argmin}_\\theta \\sum_i \\left(f_\\theta(x_i) - y_i\\right)^2\n",
    "$$\n",
    "\n",
    "Let's assume there are $n$ observations of $x_j$ with outputs $y_{j_1} \\ldots y_{j_n}$. Then, $\\hat \\theta$ is\n",
    "\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + \\sum_{k=1}^n\\left(f_\\theta(x_j) - y_{j_k}\\right)^2\\right] \\\\\n",
    "&= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + \\sum_{k=1}^n\\left(f_\\theta(x_j)^2 -2f_\\theta(x_j)y_{j_k} + y_{j_k}^2\\right)\\right] \\\\\n",
    "&= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + nf_\\theta(x_j)^2 -2f_\\theta(x_j)\\sum_{k=1}^n y_{j_k} + \\sum_{k=1}^n y_{j_k}^2\\right] \\\\\n",
    "&= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + nf_\\theta(x_j)^2 -2f_\\theta(x_j)\\sum_{k=1}^n y_{j_k}\\right]\n",
    "\\end{align}\n",
    "\n",
    "In the last equality we have dropped the term that doesn't depend on $\\theta$. We can generalize this expression to the case in which there is more than one group of tied inputs. In general, it will be\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{argmin}_\\theta \\sum_i \\left(n_i f_\\theta(x_i)^2 - 2f_\\theta(x_i)\\sum_{k=1}^{n_i} y_{i_k} \\right)\n",
    "$$\n",
    "\n",
    "with $x_i$ each unique input, $n_i$ its degeneracy and $y_{i_k}$ the corresponding $n_i$ outputs.\n",
    "\n",
    "This is the same as a reduced weighted least squares fit with the degeneracy of each input as the weights, and the average of each group of outputs as the targets:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{argmin}_\\theta \\sum_i n_i\\left(f_\\theta(x_i) - \\bar y_i\\right)^2 \\\\\n",
    "&= \\text{argmin}_\\theta \\sum_i \\left(n_i f_\\theta(x_i)^2 -2n_i f_\\theta(x_i)\\bar y_i + n_i \\left(\\bar y_i\\right)^2\\right) \\\\\n",
    "&= \\text{argmin}_\\theta \\sum_i \\left(n_i f_\\theta(x_i)^2 -2 f_\\theta(x_i)\\sum_{k=1}^{n_i} y_{i_k}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.7\n",
    "\n",
    "**Suppose we have a sample of $N$ pairs $x_i$, $y_i$ drawn i.i.d. from the distribution characterized as follows:**\n",
    "* **$x_i \\sim h(x)$, the design density**\n",
    "* **$y_i = f(x_i) + Îµ_i$, $f$ is the regression function**\n",
    "* **$\\varepsilon_i \\sim (0, \\sigma^2)$ (mean zero, variance $\\sigma^2$)**\n",
    "\n",
    "**We construct an estimator for $f$ linear in the $y_i$,\n",
    "$$\n",
    "\\hat f(x_0) = \\sum_{i=1}^N l_i(x_0; \\mathcal{X}) y_i,\n",
    "$$\n",
    "where the weights $l_i(x_0; \\mathcal{X})$ do not depend on the $y_i$, but do depend on the entire training sequence of $x_i$, denoted here by $\\mathcal{X}$.**\n",
    "\n",
    "**(a) Show that linear regression and k-nearest-neighbor regression are members of this class of estimators. Describe explicitly the weights $l_i(x_0; \\mathcal{X})$ in each of these cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression, an output is\n",
    "\n",
    "$$\n",
    "\\hat y(x_0) = x_0^T \\hat \\beta = x_0^T\\left(X^T X\\right)^{-1} X^T Y = \\left(x_0^T\\left(X^T X\\right)^{-1} X^T)\\right)_i Y_i\n",
    "$$\n",
    "\n",
    "so it is an estimator of this kind with weights\n",
    "\n",
    "$$\n",
    "l_i(x_0; \\mathcal{X}) = \\left(x_0^T\\left(X^T X\\right)^{-1} X^T\\right)_i\n",
    "$$\n",
    "\n",
    "In a k-NN method, we compute $\\hat y(x_0)$ as the mean of the outputs of the k nearest neighbors of $x_0$. Then, it is also an estimator of this kind with weights\n",
    "\n",
    "$$\n",
    "l_i(x_0; \\mathcal{X}) = \\begin{cases}\\frac{1}{k} \\text{ if $x_i \\in N_k(x_0)$} \\\\ 0 \\text{ otherwise} \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(b) Decompose the conditional mean-squared error\n",
    "$$\n",
    "E_{\\mathcal{Y} \\vert \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2\n",
    "$$\n",
    "into a conditional squared bias and a conditional variance component. Like $\\mathcal{X}$, $\\mathcal{Y}$ represents the entire training sequence of $y_i$.**\n",
    "\n",
    "There are two random variables, $\\mathcal{X}$ and $\\mathcal{Y}$ (or $\\mathcal{X}$ and $\\varepsilon$). $f(x_0)$ is a deterministic unknown function, and $\\hat f(x_0)$ is a random variable which depends on $\\mathcal{X}$ and $\\mathcal{Y}$ as\n",
    "\n",
    "$$\n",
    "\\hat f(x_0) = \\sum_{i=1}^N l_i\\left(x_0;\\mathcal{X}\\right)y_i.\n",
    "$$\n",
    "\n",
    "We assume that the mean-squared error is conditioned on $x_0$. We can compute the expected values $E\\left[\\hat f(x_0)\\right]$ and $E\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]$ (they are also conditioned on $x_0$, but I won't write it for simplicity). Using the usual trick of adding and subtracting the expected value of the random variable in the MSE, we find\n",
    "\n",
    "\\begin{align}\n",
    "E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(f(x_0) - \\hat f(x_0)\\right)^2\\right] &= E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(f(x_0) - E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right] + E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right] - \\hat f(x_0)\\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{Y}\\vert\\mathcal{X}}^2\\left[f(x_0) - E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right]\\right] + E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right] - \\hat f(x_0)\\right)^2\\right] \\\\\n",
    "&= \\text{Bias}^2\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right] + \\text{Var}\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(c) Decompose the (unconditional) mean-squared error\n",
    "$$\n",
    "E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2\n",
    "$$\n",
    "into a squared bias and a variance component.**\n",
    "\n",
    "There are two options, the obvious one is to use again the same trick as always and find\n",
    "\n",
    "$$\n",
    "E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2 = \\text{Bias}^2\\left[\\hat f(x_0)\\right] + \\text{Var}\\left[\\hat f(x_0)\\right]\n",
    "$$\n",
    "\n",
    "The other option is to compute the expected value of the conditional MSE,\n",
    "\n",
    "$$\n",
    "E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2 = E_{\\mathcal{X}}E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(f(x_0) - \\hat f(x_0)\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "For the variance term, we can use the law of total variance to get\n",
    "\n",
    "$$\n",
    "E_{\\mathcal{X}}\\left[\\text{Var}\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\\right] = \\text{Var}\\left[\\hat f(x_0)\\right] - \\text{Var}\\left[E\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\\right]\n",
    "$$\n",
    "\n",
    "For the bias term, we can add and subtract $E_\\mathcal{X}\\left[E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right]$ and proceed as usual:\n",
    "\n",
    "\\begin{align}\n",
    "E_\\mathcal{X}\\left[\\text{Bias}^2\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right] &= E_\\mathcal{X}\\left[\\left(E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0) - f(x_0)\\right]\\right)^2\\right]\\\\\n",
    "&= E_\\mathcal{X}\\left[\\left(E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0) -E_\\mathcal{X}\\left[E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right] + E_\\mathcal{X}\\left[E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right] -f(x_0)\\right]\\right)^2\\right]\\\\\n",
    "&= \\text{Var}\\left[E\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\\right] + \\text{Bias}^2\\left[\\hat f(x_0)\\right]\n",
    "\\end{align}\n",
    "\n",
    "Adding both terms we get the same result as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Establish a relationship between the squared biases and variances in the above two cases.**\n",
    "\n",
    "I don't understand the question. It could mean to establish a relationship between the conditional and unconditional squared biases and variances. In that case, I have established such a relationship in the previous answer. It could also mean to relate the biases and variances of the least squares and k-NN models. In that case, I don't think we can do much without knowing the true function $f(x_0)$. The bias depends directly on $f$, and the variance depends on $\\hat f$, but the estimator depends on the true function through the $y_i$.\n",
    "\n",
    "We could proceed if we assumed $f$ to be a linear regression (the wording of the exercise states that $f$ is a *regression function*), but both cases, the linear least squares and the k-NN, have been done in the book (Section 2.5 and Exercise 2.5 for the least squares and Section 2.9 for the k-NN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.8\n",
    "\n",
    "**Compare the classification performance of linear regression and k-nearest neighbor classification on the zipcode data. In particular, consider only the $2$â€™s and $3$â€™s, and $k = 1, 3, 5, 7$ and $15$. Show both the training and test error for each choice. The zipcode data are available from the book website [web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression will produce continuous outputs between $\\left(-\\infty, +\\infty\\right)$. To use it as a classifier, we must set a threshold. For the moment, I'm using a value of 2.5, so any value predicted to be less than 2.5 will be classified as a 2, and any value equal or greater than 2.5 will be classified as a 3.\n",
    "\n",
    "As a metric, I'll use the accuracy, which for a binary classification is \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{# of correct predictions}}{\\text{# of total predictions}}\n",
    "\\end{equation}\n",
    "\n",
    "We might argue whether this is the best possible metric. If the classes (digits 2 and 3) were equiprobable, it would be a good metric, but if there was some class imbalance, let's say more 2's than 3's, the accuracy would favour classifiers that predict more 2's. As a first approximation, we can expect the digits of a ZIP code to follow Benford's law. [For four or more digits](https://en.wikipedia.org/wiki/Benford%27s_law#Generalization_to_digits_beyond_the_first), Benford's law ensures that the distribution is approximately uniform.\n",
    "\n",
    "In a more in depth analysis we should question if the ZIP code's digits are really uniform, taking into account facts like the geographical distribution and the amount of mail that each zone receives, but we don't need that for the purposes of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression classifier\n",
      "============================\n",
      "Train accuracy:   0.9942\tTest accuracy:   0.9588\n",
      "Train error rate: 0.0058\tTest error rate: 0.0412\n",
      "\n",
      "\n",
      "K-nearest neighbors\n",
      "===================\n",
      "k = 1\n",
      "------\n",
      "Train accuracy:   1.0000\tTest accuracy:   0.9753\n",
      "Train error rate: 0.0000\tTest error rate: 0.0247\n",
      "\n",
      "k = 3\n",
      "------\n",
      "Train accuracy:   0.9950\tTest accuracy:   0.9698\n",
      "Train error rate: 0.0050\tTest error rate: 0.0302\n",
      "\n",
      "k = 5\n",
      "------\n",
      "Train accuracy:   0.9942\tTest accuracy:   0.9698\n",
      "Train error rate: 0.0058\tTest error rate: 0.0302\n",
      "\n",
      "k = 7\n",
      "------\n",
      "Train accuracy:   0.9935\tTest accuracy:   0.9670\n",
      "Train error rate: 0.0065\tTest error rate: 0.0330\n",
      "\n",
      "k = 15\n",
      "------\n",
      "Train accuracy:   0.9906\tTest accuracy:   0.9615\n",
      "Train error rate: 0.0094\tTest error rate: 0.0385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the train and test files\n",
    "train_file = './data/zipcode/zip.train.gz'\n",
    "test_file = './data/zipcode/zip.test.gz'\n",
    "\n",
    "train = np.loadtxt(train_file)\n",
    "test = np.loadtxt(test_file)\n",
    "\n",
    "# Keep only the 2's and 3's\n",
    "mask_train = np.isin(train[:, 0], [2, 3])\n",
    "mask_test = np.isin(test[:, 0], [2, 3])\n",
    "\n",
    "train = train[mask_train, :]\n",
    "test = test[mask_test, :]\n",
    "\n",
    "# Linear regression\n",
    "lm = LinearRegression()\n",
    "lm.fit(train[:, 1:], train[:, 0])\n",
    "\n",
    "threshold = 2.5\n",
    "\n",
    "predict_lm_train = lm.predict(train[:, 1:]) >= threshold\n",
    "predict_lm_test = lm.predict(test[:, 1:]) >= threshold\n",
    "\n",
    "accuracy_lm_train = accuracy_score(train[:, 0] == 3, predict_lm_train)\n",
    "accuracy_lm_test = accuracy_score(test[:, 0] == 3, predict_lm_test)\n",
    "\n",
    "# k-NN\n",
    "nneighbors = [1, 3, 5, 7, 15]\n",
    "accuracy_kn_train = dict()\n",
    "accuracy_kn_test = dict()\n",
    "\n",
    "for k in nneighbors:\n",
    "    kn = KNeighborsClassifier(k)\n",
    "    kn.fit(train[:, 1:], train[:, 0])\n",
    "    \n",
    "    predict_kn_train = kn.predict(train[:, 1:])\n",
    "    predict_kn_test = kn.predict(test[:, 1:])\n",
    "    \n",
    "    accuracy_kn_train[k] = accuracy_score(train[:, 0], predict_kn_train)\n",
    "    accuracy_kn_test[k] = accuracy_score(test[:, 0], predict_kn_test)\n",
    "\n",
    "# Print the accuracies and errors\n",
    "print('Linear regression classifier\\n============================')\n",
    "print('Train accuracy: {:8.4f}\\tTest accuracy: {:8.4f}'.format(accuracy_lm_train, accuracy_lm_test))\n",
    "print('Train error rate: {:6.4f}\\tTest error rate: {:5.4f}'.format(1 - accuracy_lm_train, 1 - accuracy_lm_test))\n",
    "print('\\n')\n",
    "print('K-nearest neighbors\\n===================')\n",
    "for k in nneighbors:\n",
    "    print('k = {}\\n------'.format(k))\n",
    "    print('Train accuracy: {:8.4f}\\tTest accuracy: {:8.4f}'.format(accuracy_kn_train[k], accuracy_kn_test[k]))\n",
    "    print('Train error rate: {:.4f}\\tTest error rate: {:5.4f}\\n'.format(1 - accuracy_kn_train[k], 1 - accuracy_kn_test[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the 1 nearest neighbor is the best classifier in the test set and, obviously, also in the training set. Higher values of $k$ reduce the accuracy.\n",
    "\n",
    "The linear classifier is the worst model. One might ask whether we could improve its accuracy by choosing a different threshold. In order to explore all possible thresholds, it's better to use a Ridge classifier, which boils down to a linear classifier when we set the $\\alpha$ parameter to $0$. This way we can use sklearn's metrics for classifiers to get a ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb33a910a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "# Linear regression classification\n",
    "lc = RidgeClassifier(alpha=0.)\n",
    "lc.fit(train[:, 1:], train[:, 0])\n",
    "\n",
    "fpr_lc, tpr_lc, thresholds_lc = roc_curve(test[:, 0], lc.decision_function(test[:, 1:]), pos_label=3)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test[:, 0] == 3, predict_lm_test).ravel()\n",
    "fpr_lm = fp / (fp + tn)\n",
    "tpr_lm = tp / (tp + fn)\n",
    "plt.plot(fpr_lm, tpr_lm, 'o', label='Threshold = 2.5')\n",
    "plt.plot(fpr_lc, tpr_lc, label='ROC curve')\n",
    "plt.legend()\n",
    "plt.xlabel('False 3\\'s rate')\n",
    "plt.ylabel('True 3\\'s rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The plot shows the ROC curve with class '3' as 'positive' and the point where the 2.5 threshold model lies. We can see that it is one of the best points in the curve, but a closer inspection reveals that either lowering or rising the threshold a little would make the model better.\n",
    "\n",
    "The 2.5 threshold is the 13th element in the arrays, since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13, 14]),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(fpr_lc == fpr_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 13]),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(tpr_lc == tpr_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the 12th and 14th elements have better accuracies than the 13th. Their true positive and false positive rates are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t12th\t13th\t14th\n",
      "TPR:\t0.9518\t0.9518\t0.9639\n",
      "FPR:\t0.0303\t0.0354\t0.0354\n"
     ]
    }
   ],
   "source": [
    "print('\\t12th\\t13th\\t14th')\n",
    "print('TPR:\\t{:6.4f}\\t{:6.4f}\\t{:6.4f}'.format(*tpr_lc[12:15]))\n",
    "print('FPR:\\t{:6.4f}\\t{:6.4f}\\t{:6.4f}'.format(*fpr_lc[12:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the 12th element, with smaller threshold than 2.5, has the same TPR but smaller FPR; while the 14th element, with higher threshold, has the same FRP but greater TPR. Since the accuracy can also be written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{# of correct predictions}}{\\text{# of total predictions}} = \\frac{\\text{TPR}\\times\\text{# of condition positive instances} + \\left(1-\\text{FPR}\\right)\\times\\text{# of condition negative instances}}{\\text{# of total predictions}}\n",
    "\\end{equation}\n",
    "\n",
    "either option has better accuracy than the 2.5 threshold. In particular, the accuracies are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t12th\t13th\t14th\n",
      "Acc:\t0.9615\t0.9588\t0.9643\n"
     ]
    }
   ],
   "source": [
    "def acc(tpr, fpr, cp, cn):\n",
    "    return (tpr*cp + (1 - fpr)*cn)/(cp + cn)\n",
    "\n",
    "cp = np.sum(test[:, 0] == 3)\n",
    "cn = np.sum(test[:, 0] == 2)\n",
    "\n",
    "accuracies = np.array([acc(tpr_lc[i], fpr_lc[i], cp, cn) for i in range(len(tpr_lc))])\n",
    "\n",
    "print('\\t12th\\t13th\\t14th')\n",
    "print('Acc:\\t{:6.4f}\\t{:6.4f}\\t{:6.4f}'.format(*accuracies[12:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 14th threshold yields the maximum accuracy amongst all possible values (although there is another threshold with the same accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. accuracy at value:  14\n",
      "# of thresholds with the max. accuracy:  2\n"
     ]
    }
   ],
   "source": [
    "print('Max. accuracy at value: ', accuracies.argmax())\n",
    "print('# of thresholds with the max. accuracy: ', np.sum(accuracies == accuracies[accuracies.argmax()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.9\n",
    "\n",
    "**Consider a linear regression model with $p$ parameters, fit by least squares to a set of training data $(x_1,y_1)\\ldots(x_N,y_N)$ drawn at random from a population. Let $\\hat\\beta$ be the least squares estimate. Suppose we have some test data $(\\tilde x_1,\\tilde y_1)\\ldots(\\tilde x_M,\\tilde y_M)$ drawn at random from the same population as the training data. If $R_\\text{tr}(\\beta) = \\frac{1}{N}\\sum_{i=1}^N\\left(y_i - \\beta^Tx_i\\right)^2$ and $R_\\text{te}(\\beta) = \\frac{1}{M}\\sum_{i=1}^M\\left(\\tilde y_i - \\beta^T\\tilde x_i\\right)^2$, prove that**\n",
    "\n",
    "$$\n",
    "E\\left[R_\\text{tr}\\left(\\hat\\beta\\right)\\right] \\leq E\\left[R_\\text{te}\\left(\\hat\\beta\\right)\\right]\n",
    "$$\n",
    "\n",
    "**where the expectations are over all that is random in each expression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call the training set $\\mathcal{T} = \\lbrace(x_1,y_1)\\ldots(x_N,y_N)\\rbrace$, and the test set $\\tilde{\\mathcal{T}} = \\lbrace(\\tilde x_1,\\tilde y_1)\\ldots(\\tilde x_M,\\tilde y_M)\\rbrace$. $\\hat\\beta$ is a random variable that depends on the training set $\\mathcal{T}$.\n",
    "\n",
    "The only difference between both MSE is that $\\hat\\beta$ depends on the training set, but is independent from the test set. To exploit this fact, we'll use two inequalities:\n",
    "\n",
    "* $E\\left[XY\\right] \\geq E\\left[X\\right]E\\left[Y\\right]$, with the equality being satisfied if $X$ and $Y$ are independent.\n",
    "* Jensen's inequality, that allows us to use $E\\left[\\left(X\\right)^2\\right] \\geq \\left(E\\left[X\\right]\\right)^2$.\n",
    "\n",
    "We also need to use the fact that $\\mathcal{T}$ and $\\tilde{\\mathcal{T}}$ are drawn from the same distribution, so any pair of training and test instances have the same expected values.\n",
    "\n",
    "Let's start by using the linearity of the expected value and the independence of each $\\left(x_i, y_i\\right)$ pair.\n",
    "\n",
    "\\begin{align}\n",
    "E_\\mathcal{T}\\left[R_\\text{tr}\\left(\\hat\\beta\\right)\\right] &= \\frac{1}{N}\\sum_{i=1}^NE_\\mathcal{T}\\left[\\left(y_i - \\beta^Tx_i\\right)^2\\right] = E_\\mathcal{T}\\left[\\left(y - \\hat\\beta^T x\\right)^2\\right] \\\\\n",
    "E_{\\mathcal{T},\\tilde{\\mathcal{T}}}\\left[R_\\text{te}\\left(\\hat\\beta\\right)\\right] &= \\frac{1}{M}\\sum_{i=1}^M E_{\\mathcal{T},\\tilde{\\mathcal{T}}}\\left[\\left(\\tilde y_i - \\beta^T \\tilde x_i\\right)^2\\right] = E_{\\mathcal{T},\\tilde{\\mathcal{T}}} \\left[\\left(\\tilde y - \\hat\\beta^T \\tilde x\\right)^2\\right]\n",
    "\\end{align}\n",
    "\n",
    "Let us expand the squares and condition the test MSE to the training set,\n",
    "\n",
    "\\begin{align}\n",
    "E_\\mathcal{T}\\left[R_\\text{tr}\\left(\\hat\\beta\\right)\\right] &= E_\\mathcal{T}\\left[y^2\\right] + E_\\mathcal{T}\\left[\\left(\\hat\\beta^T x\\right)^2\\right] - 2E_\\mathcal{T}\\left[y\\hat\\beta^T x\\right] \\\\\n",
    "E_{\\mathcal{T},\\tilde{\\mathcal{T}}}\\left[R_\\text{te}\\left(\\hat\\beta\\right)\\right] &= E_{\\mathcal{T}}E_{\\tilde{\\mathcal{T}}}\\left[\\tilde y^2 + \\left(\\hat\\beta^T \\tilde x\\right)^2 - 2\\tilde y \\hat\\beta^T \\tilde x \\vert \\mathcal{T}\\right] \\\\\n",
    "&= E_{\\tilde{\\mathcal{T}}}\\left[\\tilde y^2\\right] + E_\\mathcal{T} E_{\\tilde{\\mathcal{T}}}\\left[ \\left(\\hat\\beta^T \\tilde x\\right)^2 \\vert \\mathcal{T} \\right] - 2E_\\mathcal{T} E_{\\tilde{\\mathcal{T}}}\\left[ \\tilde y \\hat\\beta^T \\tilde x \\vert \\mathcal{T} \\right]\n",
    "\\end{align}\n",
    "\n",
    "We'll now compare both expressions term by term. The first one is the same because training and test instances are drawn from the same population\n",
    "\n",
    "$$\n",
    "E_{\\tilde{\\mathcal{T}}}\\left[\\tilde y^2\\right] = E_{\\mathcal{T}}\\left[y^2\\right]\n",
    "$$\n",
    "\n",
    "In the second one, we can use Jensen's conditional inequality in the test term,\n",
    "\n",
    "$$\n",
    "E_\\mathcal{T} E_{\\tilde{\\mathcal{T}}}\\left[ \\left(\\hat\\beta^T \\tilde x\\right)^2 \\vert \\mathcal{T} \\right] \\geq E_\\mathcal{T} \\left[\\left(E_{\\tilde{\\mathcal{T}}}\\left[ \\hat\\beta^T \\tilde x \\vert \\mathcal{T} \\right]\\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Subtracting the training term to this last expression and using $E\\left[XY\\right] \\geq E\\left[X\\right]E\\left[Y\\right]$,\n",
    "\n",
    "\\begin{align}\n",
    "E_\\mathcal{T}\\left[\\left(\\hat\\beta^T E_\\mathcal{T}\\left[x\\right]\\right)^2 - \\left(\\hat\\beta^T x\\right)^2\\right] &= E_\\mathcal{T}\\left[\\hat\\beta^T\\left(E_\\mathcal{T}\\left[x\\right] + x \\right) \\hat\\beta^T \\left(E_\\mathcal{T}\\left[x\\right] - x \\right)\\right]\\\\\n",
    "&\\geq E_\\mathcal{T}\\left[\\hat\\beta^T\\left(E_\\mathcal{T}\\left[x\\right] + x \\right) \\hat\\beta^T\\right]E_\\mathcal{T}\\left[E_\\mathcal{T}\\left[x\\right] - x\\right] = 0\n",
    "\\end{align}\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "E_\\mathcal{T} E_{\\tilde{\\mathcal{T}}}\\left[ \\left(\\hat\\beta^T \\tilde x\\right)^2 \\vert \\mathcal{T} \\right] \\geq E_\\mathcal{T}\\left[\\left(\\hat\\beta^T x\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Finally, in the third test term it is easier to swap the order of the dot product to group training and test terms,\n",
    "\n",
    "\\begin{align}\n",
    "-2E_\\mathcal{T} E_{\\tilde{\\mathcal{T}}}\\left[ \\tilde y \\hat\\beta^T \\tilde x \\vert \\mathcal{T} \\right] &= -2E_\\mathcal{T} E_{\\tilde{\\mathcal{T}}}\\left[ \\tilde y  \\tilde x^T \\hat\\beta \\vert \\mathcal{T} \\right]\\\\\n",
    "&= -2E_\\mathcal{T}\\left[E_{\\tilde{\\mathcal{T}}}\\left[ \\tilde y  \\tilde x^T\\right] \\hat\\beta\\right] \\\\\n",
    "&= -2E_{\\tilde{\\mathcal{T}}}\\left[ \\tilde y  \\tilde x^T\\right] E_\\mathcal{T}\\left[\\hat\\beta\\right] \\\\\n",
    "&= -2E_{\\mathcal{T}}\\left[ y x^T\\right] E_\\mathcal{T}\\left[\\hat\\beta\\right] \\\\\n",
    "&\\geq -2E_\\mathcal{T}\\left[y x^T \\hat\\beta \\right]\n",
    "\\end{align}\n",
    "\n",
    "In the last inequality we have used again $E\\left[XY\\right] \\geq E\\left[X\\right]E\\left[Y\\right]$. Since each of the three test terms is greater than or equal to the corresponding training term, we have proven that\n",
    "\n",
    "$$\n",
    "E\\left[R_\\text{tr}\\left(\\hat\\beta\\right)\\right] \\leq E\\left[R_\\text{te}\\left(\\hat\\beta\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that we haven't used the exact dependence of $\\hat\\beta$ on the training set anywhere. Therefore, the result is valid for any linear regression estimator, including those in which $\\hat\\beta$ is obtained by other methods different from ordinary least squares. This result is a mathematical proof for a linear regression model of the fact that we should expect the test error to be greater than the training error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
