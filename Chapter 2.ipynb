{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "**Suppose each of $K$-classes has an associated target $t_k$ , which is a vector of all zeros, except a one in the $k$th position. Show that classifying to the largest element of $\\hat y$ amounts to choosing the closest target, $$\\min_{k}\\vert\\vert t_k âˆ’\n",
    "\\hat y\\vert\\vert$$ if the elements of $\\hat y$ sum to one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between $t_k$ and $\\hat y$ is \n",
    "$$\\vert\\vert t_k - \\hat y \\vert\\vert = \\sqrt{\\sum_i\\left(t_{k_i} - \\hat y_i\\right)^2} = \\sqrt{\\sum_{i\\neq k}\\hat y_i^2 + \\left(1 - \\hat y_k\\right)^2} = \\sqrt{1+ \\sum_i \\hat y_i^2 - 2\\hat y_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $1 + \\sum_i \\hat y_i^2$ is the same for all $k$, $\\hat y$ is closest to the $t_k$ target for which $\\hat y_k$ is greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful in categorical predictions. For example, in letter recognition, we can generate a vector output of dimensionality equal to the number of letters in the alphabet and assign the input to the letter corresponding to the largest output value. It is not necessary that the elements of $\\hat y$ sum to one, but if they do we can interpret the $y_k$ value as the probability of the input to belong to class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "**Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq3QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92b13882b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed, randint, rand\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed(2666)\n",
    "\n",
    "blue_m = [1,0]\n",
    "orange_m = [0,1]\n",
    "cov = np.eye(2)\n",
    "cov_points = cov/5\n",
    "n_means = 10\n",
    "\n",
    "# Generate the 10 means for each class\n",
    "blue_means = multivariate_normal(blue_m, cov).rvs(n_means)\n",
    "orange_means = multivariate_normal(orange_m, cov).rvs(n_means)\n",
    "\n",
    "\n",
    "# Generate the 100 points for each class:\n",
    "# - Draw a mean from the 10 possible means with P=1/10\n",
    "# - Draw a point from a multivariate normal with that mean and cov = eye()/5\n",
    "blue_points, orange_points = np.empty([100, 2]), np.empty([100, 2])\n",
    "\n",
    "for i in range(100):\n",
    "    k_blue, k_orange = randint(n_means), randint(n_means)\n",
    "    blue_points[i, :] = multivariate_normal(blue_means[k_blue, :], cov_points).rvs()\n",
    "    orange_points[i, :] = multivariate_normal(orange_means[k_orange, :], cov_points).rvs()\n",
    "\n",
    "# Define the function of the Bayes optimal decision boundary\n",
    "def blue_pdf(x):\n",
    "    pdf = 0\n",
    "    for i in range(10):\n",
    "        pdf += multivariate_normal(blue_means[i,:], cov_points).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "def orange_pdf(x):\n",
    "    pdf = 0\n",
    "    for i in range(10):\n",
    "        pdf += multivariate_normal(orange_means[i,:], cov_points).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "def pdf_subtract(x):\n",
    "    p = blue_pdf(x) - orange_pdf(x)\n",
    "    return p\n",
    "\n",
    "# Compute the irreducible error\n",
    "correct = 0\n",
    "total = 3000\n",
    "for i in range(total):\n",
    "    # Select the mean\n",
    "    mean = randint(n_means)\n",
    "    if rand() > 0.5:\n",
    "        mean = blue_means[mean]\n",
    "        color = 0\n",
    "    else:\n",
    "        mean = orange_means[mean]\n",
    "        color = 1\n",
    "\n",
    "    # Draw a point\n",
    "    point = multivariate_normal(mean, cov_points).rvs()\n",
    "\n",
    "    # Predict the color\n",
    "    if pdf_subtract(point) > 0:\n",
    "        predicted_color = 0\n",
    "    else:\n",
    "        predicted_color = 1\n",
    "\n",
    "    if predicted_color == color:\n",
    "        correct += 1\n",
    "\n",
    "irr_error_rate = 1 - correct/total\n",
    "\n",
    "# Plot the means, points and Bayes boundary\n",
    "x = np.arange(-4,4,0.025)\n",
    "y = np.arange(-4,4,0.025)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "plt.plot(blue_means[:,0], blue_means[:,1], 'bx')\n",
    "plt.plot(orange_means[:,0], orange_means[:,1], 'rx')\n",
    "plt.plot(blue_points[:, 0], blue_points[:, 1], 'bo')\n",
    "plt.plot(orange_points[:, 0], orange_points[:, 1], 'ro')\n",
    "plt.contourf(X, Y, pdf_subtract(np.dstack((X,Y))), levels=[-np.Inf, 0, np.Inf],\n",
    "colors=('red', 'blue'), alpha=0.25)\n",
    "plt.xlim([-4, 3.975])\n",
    "plt.ylim([-4, 3.975])\n",
    "plt.xlabel('Irreducible error rate: ' + str(irr_error_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "**Derive equation (2.24) for the median distance from the origin to the closest data point with $N$ data points uniformly distributed in a $p$-dimensional unit ball centered at the origin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $V_p(1)$ be the volume of a $p$-dimensional unit ball. Then, the volume of a ball of radius $R$ is $V_p(R) = V_p(1) R^p$.\n",
    "\n",
    "The probability that a uniformly drawn random point will have a distance $D > d$ is proportional to the volume difference $V_p(1)(1-d^p)$. Normalizing it, we find $$P(D > d) = 1-d^p.$$\n",
    "\n",
    "Since each random point is drawn independently from the others, $$P(\\text{all $N$ distances} > d) = P(\\text{closest distance} > d) = (1-d^p)^N.$$\n",
    "\n",
    "Then, the CDF is $$P(\\text{closest distance} \\leq d) = 1 - (1-d^p)^N,$$ and the median is the value $d$ such that the CDF is $1/2$, which is $$d = \\left(1-\\frac{1}{2}^{1/N}\\right)^{1/p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we need to use $P(D > d) = 1-d^p$ because $P(\\text{all $N$ distances} > d) = P(\\text{closest distance} > d)$. With  $P(D < d) = d^p$ we could compute $P(\\text{all $N$ distances} < d) = P(\\text{furthest distance} < d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the CDF, we can also compute the mean. The PDF is (let now $x$ be the distance)\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{d}{dx} F(x) = pN\\left(1-x^p\\right)^{N-1}x^{p-1}\n",
    "\\end{equation}\n",
    "and the mean is\n",
    "\\begin{align}\n",
    "E[\\text{closest distance}] &= \\int_0^1{x f(x) dx} = \\int_0^1{pN\\left(1-x^p\\right)^{N-1}x^p dx} \\\\\n",
    "&= \\frac{N \\Gamma (N) \\Gamma \\left(\\frac{1}{p}\\right)}{p \\Gamma \\left(N+\\frac{1}{p}+1\\right)}\n",
    "\\end{align}\n",
    "\n",
    "For example, for $N = 500$ and $p = 10$, the median is 0.5178 and the mean 0.5110."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.4\n",
    "**The edge effect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution $X âˆ¼ N(0, \\mathbb{I}_p)$. The squared distance from any sample point to the origin has a $\\chi^2_p$ distribution with mean $p$.\n",
    "Consider a prediction point $x_0$ drawn from this distribution, and let $a = x_0 / \\vert\\vert x_0 \\vert \\vert$ be an associated unit vector.**\n",
    "\n",
    "**Let $z_i = a^T x_i$ be the projection of each of the training points on this direction. Show that the $z_i$ are distributed $N(0, 1)$ with expected squared distance from the origin 1, while the target point has expected squared distance $p$ from the origin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the points are drawn from a spherical multinormal distribution, each of the components of a random vector $x_i$ are normally distributed\n",
    "\\begin{equation}\n",
    "x_i^j \\sim N(0, 1).\n",
    "\\end{equation}\n",
    "But due to the spherical symmetry of the distribution, we can rotate the axes, so that any of the basis unit vectors coincides with $a$. For example, we can make $e_1 = a$. Then, \n",
    "\\begin{equation}\n",
    "z_i = a^T x_i = x_i^1 \\sim N(0, 1).\n",
    "\\end{equation}\n",
    "Since the squared norm of $k$ standard normally distributed variables is a chi-squared distribution with k degrees of freedom, with mean $k$, the expected squared distance from the origin is $1$, while the target point has expected squared distance $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hence for $p = 10$, a randomly drawn test point is about $3.1$ standard\n",
    "deviations from the origin, while all the training points are on average\n",
    "one standard deviation along direction $a$. So most prediction points see\n",
    "themselves as lying on the edge of the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.5\n",
    "**(a) Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation we want to derive is the EPE of a regression line fitted by least squares when the actual outputs are given by\n",
    "$y_i = x_i^T\\beta + \\varepsilon_i$, with $\\varepsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "Then, the training set $\\mathcal{T}$ is given by the input matrix $X$ and the outputs $Y = X\\beta + \\varepsilon$ or, equivalently, by $X$ and $\\varepsilon$. $X$ is a $N\\times p$ matrix, $\\beta$ is a $p\\times 1$ vector, and $Y$ and $\\varepsilon$ are $N\\times 1$ vectors. $N$ is the number of observations and $p$ the total number of parameters, including the constant.\n",
    "\n",
    "Since $\\hat \\beta$ is found by least squares, it is\n",
    "\\begin{equation}\n",
    "\\hat \\beta = (X^T X)^{-1}X^TY = \\beta + (X^T X)^{-1}X^T\\varepsilon\n",
    "\\end{equation}\n",
    "so we can write\n",
    "\\begin{equation}\n",
    "\\hat y_0 = x_0^T\\hat \\beta = x_0^T\\beta + \\sum_{i=1}^N l_i(x_0)\\varepsilon_i\n",
    "\\end{equation}\n",
    "with\n",
    "\\begin{equation}\n",
    "l_i(x_0) = \\left[X(X^TX)^{-1}x_0\\right]_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to know the EPE as a function of $x_0$. The random variables are the training set $\\mathcal{T}$ and the output $y_0 = x_0^T\\beta + \\varepsilon_0$. We first write the two expected values explicitly, conditioning $y_0$ on $x_0$ and use the usual trick of adding and subtracting the expected value of a random variable to obtain the variance and the bias. Following the same notation of the book, we will write the conditional expected value of a general random value as $E_{y_0 \\vert x_0}\\left[f(y_0)\\right]$ and as $E\\left[y_0 \\vert x_0 \\right]$ when $f(y_0) = y_0$.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}(x_0) &= E(y_0 - \\hat y_0)^2 = E_{y_0 \\vert x_0} E_\\mathcal{T} (y_0 - \\hat y_0)^2 \\\\\n",
    "&= E_{y_0 \\vert x_0} E_\\mathcal{T} \\left(y_0 - E(y_0 \\vert x_0) + E(y_0 \\vert x_0) - \\hat y_0\\right)^2 \\\\\n",
    "&= E_{y_0 \\vert x_0} E_\\mathcal{T} \\left[\\left( y_0 - E(y_0 \\vert x_0)\\right)^2 + \\left( E(y_0 \\vert x_0) - \\hat y_0\\right)^2  + 2\\left( y_0 - E(y_0 \\vert x_0)\\right)\\left( E(y_0 \\vert x_0) - \\hat y_0\\right) \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last term is $2E_{y_0 \\vert x_0}\\left(y_0 - E(y_0 \\vert x_0)\\right)E_\\mathcal{T}\\left(E(y_0 \\vert x_0) - \\hat y_0\\right)$ which is 0 due to the first expected value. Moreover, the first term does not depend on the training set $\\mathcal{T}$, so it becomes a variance. In the second term we can use the same trick,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}(x_0) &= E_{y_0 \\vert x_0}\\left[ y_0 - E(y_0 \\vert x_0)\\right]^2 + E_\\mathcal{T}\\left[ E(y_0 \\vert x_0) - \\hat y_0\\right]^2 \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + E_\\mathcal{T}\\left[ \\hat y_0 - E_\\mathcal{T}(\\hat y_0) + E_\\mathcal{T}(\\hat y_0) - E(y_0 \\vert x_0)\\right]^2 \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + E_\\mathcal{T}\\left[\\hat y_0 - E_\\mathcal{T}\\left(\\hat y_0\\right)\\right]^2 + E_\\mathcal{T}\\left[E_\\mathcal{T}\\left(\\hat y_0\\right) - E(y_0 \\vert x_0) \\right]^2 \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + E_\\mathcal{T}\\left[\\hat y_0 - E_\\mathcal{T}\\left(\\hat y_0\\right)\\right]^2 + E_\\mathcal{T}^2\\left[\\hat y_0 - x_0^T\\beta \\right] \\\\\n",
    "&= \\text{Var}(y_0 \\vert x_0) + \\text{Var}(\\hat y_0) + \\text{Bias}^2(\\hat y_0)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last equality we have used the definition of the bias of an estimator, $\\text{Bias}(\\hat \\theta) = E\\left[\\hat \\theta - \\theta\\right]$. It is sometimes written as $\\text{Bias}(\\hat \\theta) = E\\left[\\hat \\theta\\right] - \\theta$ but *I think* the former equality is more general, and the latter is only true if $\\theta$ is not a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is\n",
    "$$\n",
    "\\text{Var}(y_0 \\vert x_0) = \\text{Var}(x_0^T\\beta + \\varepsilon \\,\\vert\\, x_0) = \\text{Var}(\\varepsilon) = \\sigma^2\n",
    "$$\n",
    "The second one is, using the law of total variance,\n",
    "\\begin{align}\n",
    "\\text{Var}_\\mathcal{T}\\left(\\hat y_0\\right) &= \\text{Var}_\\mathcal{T}\\left(x_0^T \\hat \\beta \\right) \\\\\n",
    "&= E_X\\left[\\text{Var}_\\mathcal{\\varepsilon}\\left[x_0^T\\hat\\beta \\,\\vert\\, X\\right]\\right] + \\text{Var}_X\\left[E_\\mathcal{\\varepsilon}\\left[x_0^T\\hat\\beta \\,\\vert\\, X\\right]\\right] \\\\\n",
    "&= E_X\\left[x_0^T \\left(X^T X\\right)^{-1} x_0 \\sigma^2\\right]\n",
    "\\end{align}\n",
    "In the last equality we have used the expression of $\\hat \\beta$ in terms of $X$ and $\\varepsilon$ and Eq. (3.8) from the book. The last term vanishes because $E(\\varepsilon) = 0$.\n",
    "Finally, the last term in the expression for the EPE is\n",
    "$$\n",
    "\\text{Bias}^2\\left(\\hat y_0\\right) = \\left(E\\left[\\hat y_0\\right] - x_0^T \\beta\\right)^2 = \\left(x_0^T E\\left[\\hat \\beta\\right] - x_0^T \\beta\\right)^2\n",
    "$$\n",
    "Using again the expression for $\\hat \\beta$ in terms of $X$ and $\\varepsilon$ and conditioning on $X$,\n",
    "$$\n",
    "E\\left[\\hat \\beta\\right] = E\\left[\\beta + \\left(X^T X\\right)^{-1}\\varepsilon\\right] = \\beta + E_X\\left[E_\\varepsilon\\left[\\left(X^TX\\right)^{-1}X^T\\varepsilon \\,\\vert\\, X\\right]\\right] = \\beta\n",
    "$$\n",
    "Then, the bias is 0 and the EPE is\n",
    "\\begin{align}\n",
    "EPE(x_0) &= \\text{Var}(y_0 \\vert x_0) + \\text{Var}(\\hat y_0) + \\text{Bias}^2(\\hat y_0) \\\\\n",
    "&= \\sigma^2 + E_X\\left[x_0^T \\left(X^T X\\right)^{-1} x_0 \\sigma^2\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(b) Derive equation (2.28), making use of the cyclic property of the trace operator [trace(AB) = trace(BA)], and its linearity (which allows us to interchange the order of trace and expectation).**\n",
    "\n",
    "Remember that $X$ is an $N\\times p$ matrix,\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "x^T_1 \\\\ x^T_2 \\\\ \\vdots \\\\ x^T_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "so\n",
    "$$\n",
    "X^TX = \\begin{pmatrix}\n",
    "x_1 & x_2 & \\ldots & x_N\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x^T_1 \\\\ x^T_2 \\\\ \\vdots \\\\ x^T_N\n",
    "\\end{pmatrix} = \\sum_{i=1}^N{x_i x^T_i}\n",
    "$$\n",
    "The covariance matrix of a random variable $Z$ is defined as $\\text{Cov}(Z) = E(ZZ^T) - E(Z)E(Z^T)$. Since we are assuming $E(x) = 0$, and all the $x$ vectors are drawn from the same distribution,\n",
    "$$\n",
    "E(X^TX) = NE(x x^T) = N\\text{Cov}(x)\n",
    "$$\n",
    "\n",
    "**Warning: in general, $E(f(X)) \\neq f(E(X))$, so in principle $E\\left[(X^TX)^{-1}\\right] \\neq \\left[E(X^TX)\\right]^{-1}$, but it seems that's what they are doing in the book. It could be related to the fact that we are taking the $N\\rightarrow \\infty$ limit, but I'm not sure.\n",
    "The solution may go as follows:\n",
    "**\n",
    "$$\n",
    "E\\left[(X^TX)^{-1}\\right] = E\\left[\\left(N xx^T\\right)^{-1}\\right]\n",
    "$$\n",
    "$xx^T$ is positive definite, which *probably* means that we can use some sort of Taylor expansion with leading term $1/E(Nxx^T)$ and higher order terms suppresed by powers of 1/N. This is suggested [here](https://stats.stackexchange.com/a/80884) but there are also commments arguing against it.\n",
    "\n",
    "In any case, assuming the approximation is valid,\n",
    "\\begin{align}\n",
    "E_{x_0} EPE(x_0) &= E_{x_0}\\left[\\sigma^2 + E\\left[x_0^T\\left(X^TX\\right)^{-1} x_0 \\sigma^2\\right]\\right] \\\\\n",
    "&\\simeq \\sigma^2 + \\frac{\\sigma^2}{N}E_{x_0}\\left[x_0^T\\text{Cov}(x)^{-1}x_0\\right]\n",
    "\\end{align}\n",
    "Since $x_0^T\\text{Cov}(x)^{-1}x_0$ is a scalar, it is its own trace. Using the cyclic property of the trace and its linearity\n",
    "$$\n",
    "E_{x_0}\\left[x_0^T\\text{Cov}(x)^{-1}x_0\\right] = E_{x_0}\\left[\\text{Tr}\\left(\\text{Cov}(x)^{-1}x_0x_0^T\\right)\\right] = \\text{Tr}\\left[\\text{Cov}(x)^{-1} E_{x_0}\\left(x_0 x_0^T\\right)\\right] = \\text{Tr}\\left[\\text{Cov}(x)^{-1}\\text{Cov}(x_0)\\right]\n",
    "$$\n",
    "Since the training points $x$ and the test point $x_0$ are drawn from the same distribution, we are left with the trace over the $p\\times p$ identity matrix, which is $p$, so\n",
    "$$\n",
    "E_{x_0} EPE(x_0) \\simeq \\sigma^2 + \\frac{\\sigma^2}{N} p\n",
    "$$\n",
    "for large $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "**Consider a regression problem with inputs $x_i$ and outputs $y_i$, and a parameterized model $f_\\theta(x)$ to be fit by least squares. Show that if there are observations with *tied* or *identical* values of $x$, then the fit can be obtained from a reduced weighted least squares problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is independent of the model $f_\\theta$, as long as we fit $\\hat \\theta$ by least squares:\n",
    "$$\n",
    "\\hat \\theta = \\text{argmin}_\\theta \\sum_i \\left(f_\\theta(x_i) - y_i\\right)^2\n",
    "$$\n",
    "Let's assume there are $n$ observations of $x_j$ with outputs $y_{j_1} \\ldots y_{j_n}$. Then, $\\hat \\theta$ is\n",
    "\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + \\sum_{k=1}^n\\left(f_\\theta(x_j) - y_{j_k}\\right)^2\\right] \\\\\n",
    "&= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + \\sum_{k=1}^n\\left(f_\\theta(x_j)^2 -2f_\\theta(x_j)y_{j_k} + y_{j_k}^2\\right)\\right] \\\\\n",
    "&= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + nf_\\theta(x_j)^2 -2f_\\theta(x_j)\\sum_{k=1}^n y_{j_k} + \\sum_{k=1}^n y_{j_k}^2\\right] \\\\\n",
    "&= \\text{argmin}_\\theta \\left[\\sum_{i\\neq j}\\left(f_\\theta(x_i) - y_i\\right)^2 + nf_\\theta(x_j)^2 -2f_\\theta(x_j)\\sum_{k=1}^n y_{j_k}\\right]\n",
    "\\end{align}\n",
    "\n",
    "In the last equality we have dropped the term that doesn't depend on $\\theta$. We can generalize this expression to the case in which there is more than one group of tied inputs. In general, it will be\n",
    "$$\n",
    "\\hat \\theta = \\text{argmin}_\\theta \\sum_i \\left(n_i f_\\theta(x_i)^2 - 2f_\\theta(x_i)\\sum_{k=1}^{n_i} y_{i_k} \\right)\n",
    "$$\n",
    "with $x_i$ each unique input, $n_i$ its degeneracy and $y_{i_k}$ the corresponding $n_i$ outputs.\n",
    "\n",
    "This is the same as a reduced weighted least squares fit with the degeneracy of each input as the weights, and the average of each group of outputs as the targets:\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{argmin}_\\theta \\sum_i n_i\\left(f_\\theta(x_i) - \\bar y_i\\right)^2 \\\\\n",
    "&= \\text{argmin}_\\theta \\sum_i \\left(n_i f_\\theta(x_i)^2 -2n_i f_\\theta(x_i)\\bar y_i + n_i \\left(\\bar y_i\\right)^2\\right) \\\\\n",
    "&= \\text{argmin}_\\theta \\sum_i \\left(n_i f_\\theta(x_i)^2 -2 f_\\theta(x_i)\\sum_{k=1}^{n_i} y_{i_k}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.7\n",
    "\n",
    "**Suppose we have a sample of $N$ pairs $x_i$, $y_i$ drawn i.i.d. from the distribution characterized as follows:**\n",
    "* **$x_i \\sim h(x)$, the design density**\n",
    "* **$y_i = f(x_i) + Îµ_i$, $f$ is the regression function**\n",
    "* **$\\varepsilon_i \\sim (0, \\sigma^2)$ (mean zero, variance $\\sigma^2$)**\n",
    "\n",
    "**We construct an estimator for $f$ linear in the $y_i$,\n",
    "$$\n",
    "\\hat f(x_0) = \\sum_{i=1}^N l_i(x_0; \\mathcal{X}) y_i,\n",
    "$$\n",
    "where the weights $l_i(x_0; \\mathcal{X})$ do not depend on the $y_i$, but do depend on the entire training sequence of $x_i$, denoted here by $\\mathcal{X}$.**\n",
    "\n",
    "**(a) Show that linear regression and k-nearest-neighbor regression are members of this class of estimators. Describe explicitly the weights $l_i(x_0; \\mathcal{X})$ in each of these cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression, an output is\n",
    "$$\n",
    "\\hat y(x_0) = x_0^T \\hat \\beta = x_0^T\\left(X^T X\\right)^{-1} X^T Y = \\left(x_0^T\\left(X^T X\\right)^{-1} X^T)\\right)_i Y_i\n",
    "$$\n",
    "so it is an estimator of this kind with weights\n",
    "$$\n",
    "l_i(x_0; \\mathcal{X}) = \\left(x_0^T\\left(X^T X\\right)^{-1} X^T\\right)_i\n",
    "$$\n",
    "\n",
    "In a k-NN method, we compute $\\hat y(x_0)$ as the mean of the outputs of the k nearest neighbors of $x_0$. Then, it is also an estimator of this kind with weights\n",
    "$$\n",
    "l_i(x_0; \\mathcal{X}) = \\begin{cases}\\frac{1}{k} \\text{ if $x_i \\in N_k(x_0)$} \\\\ 0 \\text{ otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(b) Decompose the conditional mean-squared error\n",
    "$$\n",
    "E_{\\mathcal{Y} \\vert \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2\n",
    "$$\n",
    "into a conditional squared bias and a conditional variance component. Like $\\mathcal{X}$, $\\mathcal{Y}$ represents the entire training sequence of $y_i$.**\n",
    "\n",
    "There are two random variables, $\\mathcal{X}$ and $\\mathcal{Y}$ (or $\\mathcal{X}$ and $\\varepsilon$). $f(x_0)$ is a deterministic unknown function, and $\\hat f(x_0)$ is a random variable which depends on $\\mathcal{X}$ and $\\mathcal{Y}$ as\n",
    "$$\n",
    "\\hat f(x_0) = \\sum_{i=1}^N l_i\\left(x_0;\\mathcal{X}\\right)y_i\n",
    "$$\n",
    "We assume that the mean-squared error is conditioned on $x_0$. We can compute the expected values $E\\left[\\hat f(x_0)\\right]$ and $E\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]$ (they are also conditioned on $x_0$, but I won't write it for simplicity). Using the usual trick of adding and subtracting the expected value of the random variable in the MSE, we find\n",
    "\\begin{align}\n",
    "E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(f(x_0) - \\hat f(x_0)\\right)^2\\right] &= E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(f(x_0) - E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right] + E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right] - \\hat f(x_0)\\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{Y}\\vert\\mathcal{X}}^2\\left[f(x_0) - E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right]\\right] + E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\right] - \\hat f(x_0)\\right)^2\\right] \\\\\n",
    "&= \\text{Bias}^2\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right] + \\text{Var}\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(c) Decompose the (unconditional) mean-squared error\n",
    "$$\n",
    "E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2\n",
    "$$\n",
    "into a squared bias and a variance component.**\n",
    "\n",
    "There are two options, the obvious one is to use again the same trick as always and find\n",
    "$$\n",
    "E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2 = \\text{Bias}^2\\left[\\hat f(x_0)\\right] + \\text{Var}\\left[\\hat f(x_0)\\right]\n",
    "$$\n",
    "The other option is to compute the expected value of the conditional MSE,\n",
    "$$\n",
    "E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2 = E_{\\mathcal{X}}E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\left(f(x_0) - \\hat f(x_0)\\right)^2\\right]\n",
    "$$\n",
    "For the variance term, we can use the law of total variance to get\n",
    "$$\n",
    "E_{\\mathcal{X}}\\left[\\text{Var}\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\\right] = \\text{Var}\\left[\\hat f(x_0)\\right] - \\text{Var}\\left[E\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\\right]\n",
    "$$\n",
    "For the bias term, we can add and subtract $E_\\mathcal{X}\\left[E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right]$ and proceed as usual:\n",
    "\\begin{align}\n",
    "E_\\mathcal{X}\\left[\\text{Bias}^2\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right] &= E_\\mathcal{X}\\left[\\left(E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0) - f(x_0)\\right]\\right)^2\\right]\\\\\n",
    "&= E_\\mathcal{X}\\left[\\left(E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0) -E_\\mathcal{X}\\left[E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right] + E_\\mathcal{X}\\left[E_{\\mathcal{Y}\\vert\\mathcal{X}}\\left[\\hat f(x_0)\\,\\vert\\mathcal{X}\\right]\\right] -f(x_0)\\right]\\right)^2\\right]\\\\\n",
    "&= \\text{Var}\\left[E\\left[\\hat f(x_0) \\,\\vert\\mathcal{X}\\right]\\right] + \\text{Bias}^2\\left[\\hat f(x_0)\\right]\n",
    "\\end{align}\n",
    "Adding both terms we get the same result as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Establish a relationship between the squared biases and variances in the above two cases.**\n",
    "\n",
    "I don't understand the question. It could mean to establish a relationship between the conditional and unconditional squared biases and variances. In that case, I have established such a relationship in the previous answer. It could also mean to relate the biases and variances of the least squares and k-NN models. In that case, I don't think we can do much without knowing the true function $f(x_0)$. The bias depends directly on $f$, and the variance depends on $\\hat f$, but the estimator depends on the true function through the $y_i$.\n",
    "\n",
    "We could proceed if we assumed $f$ to be a linear regression (the wording of the exercise states that $f$ is a *regression function*), but both cases, the linear least squares and the k-NN, have been done in the book (Section 2.5 and Exercise 2.5 for the least squares and Section 2.9 for the k-NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
